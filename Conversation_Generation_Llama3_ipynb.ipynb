{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garik714/call-center-conversation-generation/blob/main/Conversation_Generation_Llama3_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqlDk4l1mnKI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 1 – Install\n",
        "!pip install -q transformers[torch] datasets accelerate evaluate rouge_score bitsandbytes peft\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 – Imports\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import evaluate\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZK_1ZMbhnDgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 – Mount Drive & Extract\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Machine Learning Researcher_Task.zip\"  # Adjust if needed\n",
        "extract_path = \"/content/task_data\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"✅ Extraction complete\")\n"
      ],
      "metadata": {
        "id": "QlzHrIphnDrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 – Load Dataset\n",
        "base_path = \"/content/task_data\"\n",
        "data_files = {\n",
        "    \"train\": f\"{base_path}/train.json\",\n",
        "    \"validation\": f\"{base_path}/validation.json\",\n",
        "    \"test\": f\"{base_path}/test_summary_only.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "4TXx047UnDvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 – Format Data for Mistral\n",
        "def format_conversation(example):\n",
        "    prompt = f\"<s>[INST] Generate a customer service conversation (3-6 turns) from this summary.\\nClient speaks first, Agent responds professionally.\\n\\nSummary: {example['summary']} [/INST]\\n\\n{example['conversation']}</s>\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "def format_test(example):\n",
        "    prompt = f\"<s>[INST] Generate a customer service conversation (3-6 turns) from this summary.\\nClient speaks first, Agent responds professionally.\\n\\nSummary: {example['summary']} [/INST]\"\n",
        "    return {\"prompt\": prompt}\n",
        "\n",
        "train_data = dataset[\"train\"].map(format_conversation)\n",
        "val_data = dataset[\"validation\"].map(format_conversation)\n",
        "test_data = dataset[\"test\"].map(format_test)"
      ],
      "metadata": {
        "id": "wZPyQjronDzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "#  Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "# Attach LoRA adapters to the quantized model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "EhHzi1vznD3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 – Tokenize\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False)\n",
        "\n",
        "tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\", \"summary\", \"conversation\"])\n",
        "tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=[\"text\", \"summary\", \"conversation\"])"
      ],
      "metadata": {
        "id": "xOzXsudwnD9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 – Data Collator and Training Arguments\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "Xfr4O4QxnEAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 – Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "U_3RgjjsnEDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 – Generate Test Conversations\n",
        "def generate_conversation(prompt, max_new_tokens=300):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_beams=3\n",
        "        )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"[/INST]\" in generated:\n",
        "        generated = generated.split(\"[/INST]\")[-1].strip()\n",
        "    return generated\n",
        "\n",
        "test_prompts = test_data[\"prompt\"]\n",
        "test_ids = dataset[\"test\"][\"id\"]\n",
        "generated_convs = []\n",
        "for i, (prompt, sid) in enumerate(zip(test_prompts, test_ids)):\n",
        "    print(f\"Generating {i+1}/{len(test_prompts)}...\")\n",
        "    conv = generate_conversation(prompt)\n",
        "    generated_convs.append({\"id\": sid, \"conversation\": conv})"
      ],
      "metadata": {
        "id": "G2zO2Ba5nEF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 – Save & Download\n",
        "with open(\"generated_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(generated_convs, f, indent=2, ensure_ascii=False)\n",
        "print(\"✅ File saved as generated_test.json\")\n",
        "from google.colab import files\n",
        "files.download(\"generated_test.json\")\n",
        "!cat generated_test.json"
      ],
      "metadata": {
        "id": "gUWh4-8dnEIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "with open(\"generated_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def clean_conversation(text):\n",
        "\n",
        "    marker = \"\\n\\nClient:\"\n",
        "    if marker in text:\n",
        "\n",
        "        cleaned = \"Client:\" + text.split(marker, 1)[1]\n",
        "    else:\n",
        "\n",
        "        if \"Client:\" in text:\n",
        "            cleaned = text[text.index(\"Client:\"):]\n",
        "        else:\n",
        "            cleaned = text  # fallback\n",
        "\n",
        "\n",
        "    cleaned = cleaned.split(\"#\")[0].strip()\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "for item in data:\n",
        "    item[\"conversation\"] = clean_conversation(item[\"conversation\"])\n",
        "\n",
        "\n",
        "with open(\"generated_test_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Cleaned file saved as 'generated_test_cleaned.json'\")\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_test_cleaned.json\")\n",
        "!cat generated_test_cleaned.json"
      ],
      "metadata": {
        "id": "Hrin5FqiHG0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kz8QG9oNnELj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}